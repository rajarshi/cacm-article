\documentclass{sig-alternate}

\usepackage{array}
\usepackage{pifont}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}

\newcommand{\none}{\ding{55}}
\newcommand{\least}{\ding{51}}
\newcommand{\little}{\ding{51}\ding{51}}
\newcommand{\lots}{\ding{51}\ding{51}\ding{51}}


\begin{document}
\pagenumbering{arabic}


\title{Cheminformatics: The Computer Science of Chemical Discovery}
\numberofauthors{9}
\author{
\alignauthor
Joerg Kurt Wegner\\
       \affaddr{Tibotec BVBA}\\
       \affaddr{Turnhoutseweg 30}\\
       \affaddr{2340 Beerse Turnhout, Belgium}\\
       \email{jwegner@its.jnj.com}
% 2nd. author
\alignauthor
Aaron Sterling\\
       \affaddr{Department of Computer Science}\\
       \affaddr{Iowa State University}\\
       \affaddr{Ames, Iowa, USA}\\
       \email{sterling@iastate.edu}
% 3rd author
\alignauthor
Rajarshi Guha\\
\affaddr{NIH Center for Translational Therapeutics}\\
\affaddr{9800 Medical Center Drive}\\
\affaddr{Rockville, MD 20850}\\
\email{guhar@mail.nih.gov}
}

\additionalauthors{Additional authors:
Andreas Bender (University of Cambridge, email: {\texttt{andreas.bender@cantab.net}}),
Jean-Loup Faulon (University of Evry, email: {\texttt{Jean-Loup.Faulon@issb.genopole.fr}}),
Janna Hastings (European Bioinformatics Institute, Cambridge, UK, email: {\texttt{janna.hastings@gmail.com}}),
Noel O'Boyle (University College Cork, Cork, Ireland, email: {\texttt{baoilleach@gmail.com}}),
John Overington (European Bioinformatics Institute, Cambridge, UK, email: {\texttt{jpo@ebi.ac.uk}}),
Herman Van Vlijmen (Tibotec, Beerse, Belgium, email: {\texttt{hvvlijme@its.jnj.com}}), and
Egon Willighagen (Karolinska Institutet, Stockholm, Sweden, email: {\texttt{egon.willighagen@ki.se}})
.}
\date{25 June 2011}


\maketitle
\begin{abstract}
  One of the most prominent success stories in all the sciences over
  the last decade has been the advance of bioinformatics: The
  interdisciplinary collaboration between computer scientists and
  molecular biologists led as one of its major accomplishments to the
  sequencing of the human genome, where researchers connected
  biological concepts to the theory of string algorithms. However,
  despite this great success, few computer scientists are familiar
  with a related discipline: cheminformatics, the use of computers to
  represent the structures of small molecules and analyze their
  properties.  Cheminformatics has wide applicability, from the
  discovery of novel drug candidates to the optimization of certain
  physicochemical properties of small molecules. Until recently, much
  of the the data and some of the techniques employed in the
  cheminformatics field have been closely guarded secrets of companies
  whose financial success depended on being the first to produce the
  new therapeutic molecules.  Only within the last decade -- and as an
  effect of a change in mindsets, government-mandated policy changes
  and last but not least because of chemists volunteering their time
  for an Open Science ``movement'' -- have researchers gained access to
  freely available software packages and databases of tens of millions
  of chemicals. Academic chemists now confront a variety of unsolved
  algorithmic problems that could not have been tackled a decade ago,
  but whose solutions are critical to research ranging from
  determining the behavior of small molecules in biological pathways,
  to finding therapies for rare and neglected diseases.
\end{abstract}

\category{J.2}{Computer Applications}{Physical Sciences and Engineering}[Chemistry]
\terms{Algorithms, Design, Human Factors, Theory}
\keywords{cheminformatics, chemoinformatics, graphs, chemistry, databases, knowledge management, open source, open
data}

\section{Is Cheminformatics the New \\Bioinformatics?}

Novel technologies in the life sciences churn out information at an
ever increasing rate -- public data stores such as the one at the
European Bioinformatics Institute (EBI) contain on the order of 10
petabytes of biological information.  While biological information has
been publicly available for decades, the same has not been true for
chemical information, until recently. It was not until 2004 that a
large public small molecule structure repository (PubChem) became
freely available. This was followed by other databases, as discussed
later in this article. While much of the foundational algorithms of
cheminformatics have been described since the 1950s, publically
available software implementing many of these algorithms have only
been accessible in the last ten to fifteen years \cite{faulon2010}.

So why should we actually care about chemical information being made
public -- and how does this relate to the field of computer science?

Making chemical information public is important because the drugs we
take are the result of intensive and costly research, which is
facilitated by the amount of information available. The more
information is shared (a process very hard to achieve in
pharmaceutical companies), the more information is available to each
researcher, facilitating the development of novel treatments. The
relevance of computer science to this research becomes clear when we
consider the quantity and types of data available -- currently a
single data base such as PubChem contains more than 34 million
chemical structure records, along with an even larger number of
annotations such as synonyms, known targets, mode of action,
regulatory approval status and so on.  Simply considering the synonym
list, PubChem stores 50.1 million synonym for 19.6 million compounds.

The design of data structures for this chemical information, as well
as subsequent data mining of this information, are areas where
expertise in computer science is urgently needed. (For a recent,
detailed introduction to cheminformatics, written for computer
scientists, see \cite{brown2009}).

Cheminformatics comprises different areas, which broadly speaking can
be divided into three fields: \emph{capturing data} (using lab
notebooks or potentially using formats such as the Chemical Markup
Language for publications); \emph{storing data} (designing database
schemas, devising ontologies) and \emph{mining data} (such as for
bioactivity prediction, which might involve algorithms based on
graphs).  However, the type of information being analyzed is different
from biological information: while bioinformatics often deals with
sequences, the domain of cheminformatics is chemical structures. In
the case of the former, information can very frequently be represented
as one-dimensional strings, which are relatively easy to handle
computationally. In the case of the latter however, chemical
structures may possess rings, branches as well as multiple valid
representations of the same structure (such as so-called
``tautomers'', where hydrogen atoms can be positioned in different
places of the molecules), potentially giving rise to
ambiguities. Hence, chemical structures are often reported to be more
difficult to standardize.

To illustrate the difficulty of standardizing chemical structures,
different representations of the same chemical structure are shown in
Figure~\ref{figure:chemical-structures}. The representations range
from a graph (Fig.~\ref{figure:chemical-structures}A) that contains
much implicit information (such as unlabeled nodes represent carbons
with valencies satisified by hydrogens) to a fully labeled graph,
where all information is explicit
(Fig.~\ref{figure:chemical-structures}B). The former is what is
usually exchanged informally between chemists, but to actually mine
chemical structures computationally, something like the latter is
required.  For example, graph mining algorithms can be applied to
labeled graph representations, to discover patterns in large-scale
datasets \cite{wegner2006,horst2009}.

\begin{figure}
\centering
\includegraphics[height=2in]{chemstructs.png}
\caption{A hierarchy of chemical structure representations, increasing
  in information content from left to right and top to bottom. A
  chemists traditional 2D depiction (A), a labeled graph (B), a 3D
  conformation (C) and a surface represention (D). All show differing
  (and valid) aspects of the same molecule and are suited for
  differing purposes. The choice of representation is guided by the
  intended use of the structure data.}
\label{figure:chemical-structures}
\end{figure}

Although graphs implicitly encodes the 3D structure of a molecule
(when combined with knowledge about favored bond angles and
distances), many different low energy 3D structures, or conformers,
may be consistent with the same graph structure. In addition, the 3D
structure may have a ``secondary'' geometrical arrangement of features
(such as the presence of a right-handed helix) which cannnot be
encoded in the graph. Thus we have 3D representations
(Fig.~\ref{figure:chemical-structures}C) that make the 3D arrangement
of atoms and bonds explicit. Fig.~\ref{figure:chemical-structures}D
goes further and represents a molecular surface, encoding some
property (such as lipophilicity) that varies over the surface.

Even though some structure representations contain more explicit
information than others, they are all equally valid and are suited for
different problems. Indeed, Fig.~\ref{figure:chemical-structures}
represents a hierarchy of representations. Thus, when searching for
substructures, a 2D representation is sufficient. When trying to
rationalize why a molecule binds to a protein, a 3D structure is
vital.

Given that a fundamental principle of the field is that \emph{similar
  molecules exhibit similar properties} \cite{Johnson:1990qf}, the
choice of representation is key in determining how such similarities
are evalaued, and thus the effectiveness of subsequent analyses. But
even after one has selected a representation one faces the challenge
of balancing computational costs with the utility of the
representation. For example, a full 3D description of a molecule,
taking into account all possible conformers would let us accurately
predict many properties. But the size of this representation and the
time required to evaluate it would be prohibitive. Instead, can we
obtain accurate predictions from a subset of conformers? Or, can we
obtain comparable accuracy by using a 2D representation? And is so,
what type of labels are required? Currently, many of these questions
are answered using trial and error.

From the above it becomes clear that the nature of chemical problems
is often a complex mixture of combinatorial and continuous
problems. If we oversimplify the nature of chemical structures and
their properties, we can not describe the world around us in
sufficient detail. On the other hand, if we try to encode every detail
(\emph{e.g.}, all possible 3D structures and their properties, all
metabolites of a structure in humans in different tissues), the
complexity of collecting, storing, and mining these data increases
dramatically. For the field of cheminformatics to flourish, we need
closer collaboration between chemists (or, more generally, life
scientists) and computer scientists -- where the former need to be
able to pose their problems in a way relevant for practical
applications; and where the latter are able to devise ways of
capturing, storing, and analyzing chemical data which achieve optimal
balances between the different risks in generalizable encodings and
complexity (space and time). The goals of cheminformatics are to
support better chemical decision making by 1. storing and integrating
data in maintainable ways, 2.  providing enough open standards and
tools so software engineering concepts allow applications and data to
be used across heterogeneous platforms, and 3. mining the many
chemical property spaces in a time- and space-efficient way. We will
discuss all three goals in the following sections.

\section{Bridging Cheminformatics and Computer Science}

Our motivation for this article is to highlight a variety of topics in
cheminformatics where modern CS research could contribute, and
to present the computational and algorithmic problems
that are addressed in the field.

While cheminformatics has a broad applicability, ranging from
agrochemical research and drug discovery to the design of novel
materials, we have chosen to use drug discovery as the context of this
article due to its central relevance for human wellbeing. Given that
the safety of drugs on the market is a key concern both for the
general audience as well as researchers in pharmaceutical companies,
the remainder of this article is structured around the concept of
``risk minimization'' in drug discovery -- the question of how to
minimize the chances of a small molecule failing, due to poor
physical, chemical or biological properties, in the various research
and development stages as a drug candidate, and how to successfully
employ cheminformatics methods in these efforts. As an example, a
molecule must be soluble and show a certain degree of bioavailability
to be considered as drug candidate. In cases where these properties
are poor, a cheminformatics approach can suggest replacement of
certain functional groups, that maintain potency but improve the
solubility and bioavailability.  Table~\ref{table:properties} (in the
Appendix) summarizes a number of properties a drug candidate must
satisfy to be considered therapeutically useful (such as being
appropriately soluble, exhibiting minimal toxicity and being selective
for the intended target) and sketches the role of cheminformatics at
each stage of drug development.
%
\subsection{Databases \& Ontologies}
\label{sec:databases}
%
Most cheminformatics applications rely on large databases of chemical structures,
their properties, and relationships to, \textit{e.g.}, biological targets.
Organizing and maintaining such databases, as well as searching and clustering similar structures together,
are essential  to enable many scientific applications. Yet, each one of
those areas poses its unique computer science challenges.

There is considerable progress in open chemical and bioactivity databases like
ChEMBL and NCBI-PubChem, which are both freely available and contain in the order of millions (ChEMBL) and tens of
millions (PubChem) of data points each. Still, the integration
of chemical databases with each other remains a challenge, not only due to the
normalization problems in the chemical and bioactivity landscape, but also due
to sheer data volume. One problem is the annotation with the ``protein targets'' a molecule binds to -- here dozens of
protein and gene identifiers exist, generally without the availability of a one-to-one mapping from each ontology, which makes merging databases often a cumbersome exercise.

Given the requirement to ``make sense'' of as much chemical data at
hand as possible, we have to be able to search in complex data, for
example multi-labelled molecular graphs, where graph labels can be
different. Hence, we need to be able to capture molecular graphs in a
risk-bounded machine encoded way.

A fundamental challenge in cheminformatics is the unique
representation of chemical structures in a machine readable format. A
(chemical) graph can be encoded in multiple ways, depending on how one
orders the nodes, resulting in multiple representations of the same
molecule. When one considers the need to include other aspects such as
protonation and tautomer states, the challenge increases. The need for
a unique representation that is invariant with respect to the atom
ordering arises due to the fact that graph isomorphism (i.e., checking
whether two molecules are the same) is an expensive operation. The
first such canonicalization algorithm was described by Morgan
\cite{Morgan1965}, allowing one to generate unique string
representations of chemical graphs letting one compare structures via
string comparisons. The SMILES (Simplified Molecular Input Line
Specification) format \cite{Weininger:1988kx} is an example of a
representation that can be canonicalized. Since the original
canonicalization algorithm was proprietary, multiple implementations
of the format have become available, each one employing a different
canonicalization algorithm, usually based on the Morgan algorithm. See
Warr (Warr \cite{Warr:2011vn}) for an extensive dicussion on chemical
structure representations.

With each database using their own algorithm for molecular encoding,
automated exchange of data between different databases was hindered.
As more and more data became available online, the use of unique
structure-based identifiers became a pressing need, resulting in the
IUPAC International Chemical Identifier (InChI) to be developed. The
InChI is a non-proprietary, structured textual identifier for chemical
entities~\cite{inchi}. InChI identifiers are not intended to be read
and understood by humans, but are useful for computational matching of
chemical entities. For rapid database lookups, the InChIKey is a
hashed key for the InChI, which has an invariant length of 14
characters. (Unfortunately, after implementation and publication of
the hashing scheme it became apparent that in particular in case of
stereochemically related molecules clashes of the InChI key take place
in some, albeit rare cases.) Figure~\ref{figure:smiles} illustrates
the SMILES, InChI and InChIKey for lipoic acid.

\begin{figure}
\centering
\includegraphics[height=1in]{lipoicacid.png}
\caption{The standard chemical graph illustration together with the SMILES, InChI and InChIKey codes for lipoic acid. Note that the SMILES is much more human-readable than the others.}
\label{figure:smiles}
\end{figure}

The InChI is now widely used for matching identical chemical
structures, but it is still limited. For example, it cannot
differentiate between certain types of stereoisomers:
Figure~\ref{figure:cistrans} (in the Appendix) illustrates two
stereoisomers for which the generated InChI is the same.

InChI or other identity-mapping algorithms allow for exact searching. However, two other
practically relevant algorithms for scientific discovery based on
chemical databases are \emph{substructure searching} and
\emph{similarity searching}, which are required to generalize from the
data points contained in a particular database to other, related
molecules. In substructure searching, the database is searched for a  
specified wholly contained part of the structures in the database,
while in similarity searching, the database is searched for structures
similar (in some property space) to a provided search, or query,
structure. Chemical search packages are often implemented and
optimized for a given database technology; for example, the OrChem
package is an open source chemical search package for the Oracle
database \cite{rijnbeek2009}.

Graph substructure matching is a variant of \emph{graph isomorphism},
which is widely believed to be computationally
intractable~\cite{cordella2001}. To execute a graph isomorphism search
across a full chemical database of thousands or even millions of
structures is infeasible \cite{Weininger:2011ly}. Speedups can be
obtained via the use of heuristics such as structural
\emph{fingerprints}. Fingerprints encode characteristic features of a
given chemical structure, usually in a fixed-length
bitmap. Fingerprints fall broadly into two categories: structure keys
and hashed keys. In the former, each bit position corresponds to a
distinct substructure such as an alcohol or keto group. Examples
include MACCS and PubChem keys. The latter class are designed such
that substructural patterns are represented as strings and then hashed
to a random bit position. As a result, a given position can encode
multiple substructures. The advantage of such fingerprints is that
they can cover an arbitrarily large collection of substructures such
as paths of length $N$ or circular environments and so on. Examples
include ECFPs and Daylight fingerprints.

Given a binary fingerprint, we can first ``pre-screen'' a database, to
ignore molecules that cannot possibly match the query. This is
achieved by requiring that all bits in a query fingerprint must also
be present in the target fingerprint. Since the target fingerprints
are pre-computed, comparing binary strings can be performed extremely
rapidly on modern hardware. As a result, we apply the actual
isomoprhism test only on those molecules that pass the
screen. Fingerprints can also be used to rapidly search databases for
similar molecules, in which case a similarity metric such as the
Tanimoto coefficient is used to compare the query and target
fingerprints. While fast, heuristics have been developed
\cite{Swamidass:2007ve} to further
speed up similarity searches.

%Fingerprints also facilitate the rapid calculation of quantitative
%structure-based similarity measures. An example measure of similarity is the
%\emph{Tanimoto coefficient}, calculated as the ratio $T(a,b) = \frac{c}{(a+b-c)}$,
%where $c$ is the count of bits on in the same position in both the two
%fingerprints, $a$ is the count of bits on in the first structure, and $b$ is the
%count of bits on in the second structure. Historically, this coefficient originated from the comparison of ecosystems
%around lakes, where a measure was needed to describe how similar those ecosystems are with respect to the animals,
%plants etc. they contain. However, it can be applied to every other system as well where features can be described in a
%binary manner. The Tanimoto coefficient varies in the
%range 0.0 -- 1.0, with a score of 1.0 indicating that the two structures are either very
%similar or even identical (i.e. their fingerprints are the same).
%
Structure-based similarity is however of limited use in grouping of
chemicals based on non-structural features, such as shared bioactivity
profiles. For this purpose, a large-scale and flexible classification
structure is needed for chemicals and their linked biological
information. One such answer to this need is provided in the form of
\emph{ontologies}.  An ontology is a formal specification of entities
and their relationships in a domain of interest. It is structured
around an underlying logical representation such as Description
Logics~\cite{baaderdl2007}. The logic-based representation allows
automated reasoning to derive new knowledge (inferences) from the
knowledge encoded, making the representation both compact and
powerful.  Ontologies are already in widespread use across the
biomedical domain, and in the cheminformatics field ontologies
relating to the chemical structures themselves and the protein targets
the molecules are active against are probably most common. A
similarity measure based on the information encoded in ontologies is
termed \emph{semantic similarity}, and such measures have already been
applied to enhance chemical classification in several problem areas
\cite{couto2010}. Many open questions still remain about the best way
to combine the information encoded in chemical structures with
chemical ontologies \cite{hastingsowled2010}.

\subsection{Structure Enumeration}
\label{sec:struct-enum}

Enumerating molecules is a combinatorial problem that has fascinated
chemists, computer scientists and mathematicians alike, for more than
a century. It is one important risk-reduction element in still being
able to encode closely-related molecules and for keeping the space
complexity of databases under (algorithmic) control. Indeed, while
trying to solve the problem of counting the isomers of paraffin
structures or counting substituted aromatic compounds, fundamental
principles in graph theory and combinatorics were developed by Cayley,
Polya and others. Even terms that are widely-used nowadays, like
``graph'' and ``tree'', were originally coined in a chemistry context
(Chapters 1 and 8 in \cite{faulon2010}). More than four decades ago,
Joshua Lederberg, Carl Djerassi, Edward Feigenbaum and others from
Stanford University started developing codes to explicitly enumerate
molecules. DENDRAL is generally quoted in artificial intelligence
textbooks as a pioneer project that produced the first expert
system~\cite{DENDRAL}. 

Enumerating molecules is not only an interesting academic exercise but
has practical utility as well. The foremost application of enumeration
is \emph{structure elucidation} of natural compounds such as
metabolites. Metabolites are small molecules that can be found an
organism (such as a human) and which in many cases indicate the state
the organism is in -- such as indicating a disease, or a fasted state,
etc. -- and with a sufficiently large database the idea is to be able
to detect the chemical composition of body fluids and to arrive at
conclusions regarding the particular state of the organism at this
point in time.  Ideally, the wishful bench chemist collects
experimental data from an unknown compound, the data is fed into a
code, and the resulting structure is returned. Although that
streamlined process may not yield a unique solution, there are
commercial software products such as MOLGEN (\url{http://molgen.de})
that can, for instance, list all structures matching a given molecular
formula, or mass spectrometry (MS) data. Another important application
of enumeration is \emph{molecular design}. Here the problem is to
design compounds (drugs, for instance) that optimize some physical,
chemical, or biological property or activity. Although less prolific
and more recent than structure elucidation, molecular design has
introduced some novel solutions to molecular enumeration. Finally,
with the advent of \emph{combinatorial chemistry} (the rapid synthesis
of different but related molecules), molecular enumeration has taken a
central role as it allows computational chemists to construct virtual
libraries and test hypotheses, and it provides guidance in the design
of optimal combinatorial experiments.

The major difficulty with enumeration (apart from the size of the
search space, which is thought to be in the order of $10^{60}$
molecules for 30 heavy atoms \cite{Bohacek:1996ve}) is that the
\emph{in silico} representation of molecules, the so-called molecular
graphs, are labeled objects (i.e. labeled graphs), while the atoms of
a molecule are of course not uniquely labeled. The mathematical
concept to tackle this problem is to consider orbits of labeled
molecular graphs under the operation of the symmetric group. While the
complexity of molecular graph enumeration remains an open problem it
has nonetheless been shown that molecules can be sampled
efficiently \cite{goldberg1999}. Sampling procedures based on the
Metropolis or the Genetic Algorithms have been developed and used to
elucidate natural compounds from NMR data.

The underlying computational complexity and potential intractability
of molecular enumeration may not be a bottleneck after all, as
computational chemists have developed and successfully used
enumeration tools to generate large chemical libraries such as GDB-13,
comprising almost a billion chemical structures~\cite{GDB}. Yet, the
current enumeration software products do not generally produce
stereoisomers or tautomers; these require specific enumeration
procedures, which are still the subject of investigations by the
cheminformatics community. In addition, GDB-13 only enumerates
molecules up to 13 heavy atoms -- given the possibly exponential
increase in search space with the number of atoms, it seems unlikely
that the practically relevant chemical space of about 30 heavy atoms
will be exhaustively enumerated in the near future.

Apart from analytical chemistry and compound design, enumeration plays
a role is in the \emph{generation of chemical reaction networks}. The
problem here consists of enumerating all the possible compounds that
can be produced by applying reaction rules to a set of initial
molecules. By reversing the reaction rules one can also find sets of
starting compounds necessary for producing a given target. This latter
process is known in chemistry as \emph{retrosynthesis}, ever since the
works of E.J. Corey, who received the Nobel Prize in Chemistry in
1990. Designing new drugs or chemicals, understanding the kinetics of
combustion and petroleum refining, studying the dynamics of metabolic
networks, applying metabolic engineering and synthetic biology to
produce heterologous compounds in microorganisms, all are applications
that require the enumeration of reaction networks. As reviewed in
Chapter 11 in \cite{faulon2010} several network enumeration techniques
have been developed. However, these techniques generally suffer from a
combinatorial explosion of intermediate compounds being produced. One
way to limit the number of compounds generated is to simulate the
dynamics of the network while it is being constructed and remove
compounds of low concentration. Following that idea, methods have been
developed based on the Gillespie Stochastic Simulation Algorithm (SSA)
to compute on-the-fly species concentrations. Chemical reaction
network enumeration and sampling is an active field of research,
particularly in the context of metabolism, either to study
biodegradation, or to propose metabolic engineering strategies to
biosynthesize compounds of commercial interest. The difficulty with
metabolic network design is that in addition to network generation
based on reactions, one also needs to verify that there are enzymatic
events possible for enabling reactions catalyses. That additional task
requires encompassing both chemical and sequence information and the
development of tools that are at the interface between cheminformatics
and bioinformatics.

\subsection{Activity Mining \& Prediction}
\label{sec:activity-mining}


The basis of predictive modeling in cheminformatics is that the
biological activity of a molecule is a function of its chemical
structure. Together with the \emph{similarity property principle}
\cite{Johnson:1990qf} (structurally similar molecules will exhibit  
similar properties), the goal of any modeling approach is to capture
and characterize the correlations between structural features and the
observed biological activities. Simultaneously, such approaches must
also describe the likelihood of error when using such
models for decision making.  A variety of approaches can be employed
to assess the error in (or conversely, the reliability or confidence of) a
prediction, ranging from statistical approaches (XXX) to more
empirical approaches such as defining an \emph{applicability domain} -
the region of the input (i.e., chemical structures) that can be
reliably predicted, usually deliniated using some form of similarity
to the training set.

In many cases, the activity of a small molecule is due to its
interaction with a receptor. Traditionally, QSAR' \cite{Hansch:1962vn,
  Free:1964ys} approaches do not take into account receptor features,
focusing only on small molecule features, and therefore lose valuable
information on ligand-receptor interactions. As a result, techniques
such as docking, pharmacophore modeling and proteochemometric methods
have been designed to take into account both ligand and receptor
structures. The last method is an example of an extension of
statistical QSAR methods to simultaneous model the f receptor and
ligand as first reported by Lapinsh \textit{et  al.}~\cite{lapinsh2001}.

The first step in the predictive modeling of biological activities is
to describe compounds in a way amenable to computerized data mining,
while at the same time capturing information relevant for the property
one attempts to model. Here multiple approaches exist: Either we
transform molecular graphs into a numerical vector representation
(a.k.a. \emph{descriptors} or \emph{features}) of chemical structures,
or we use algorithms for comparing molecular graphs directly
(\emph{molecule kernel}). In the first case thousands of descriptors
(many of which are equivalent) have been described
\cite{todeschini2000}, and a variety of tools are available for their
calculation. Given a set of molecules, observed activities and a
descriptor vector, the problem can be considered a traditional
statistical modeling problem. In the second case no explicit vector
representation is required, which prevents the challenge of
transforming the molecular graph into a vector without losing
information. On the other hand, the implicit kernel encoding imposes
the challenge that we have to compare all compounds against all
compounds, which imposes large-scale mining challenge, which risks
overrunning available processing time and space. Note that the public
PubChem dataset alone contains over 30 million chemical compounds and
over 70 million substances.

In order to judge which predictive modeling approach is applicable in
every case, we firstly have to consider the goal of a the model in a
cheminformatics setting. In some cases, pure predictive ability is
desired -- such as in a virtual screening setting. (Basically, only
the reliability of the model matters, but other aspects such as
interpretability are of minor importance.) For such cases, the
complexity of the modeling approach is immaterial, as it leads to
accurate and generalizable results. Here a large-scale mining capacity
is crucial. On the other hand, when working with chemists and
biologists, the interpretability of a model and risk reduction
arguments are paramount, since we have to translate better-predicted
\emph{in silico} compounds into real-world compounds. In such cases,
``black-box methods'' can be less useful, though attempts have been
made to interpret vectorial models built using neural networks,
na\"{i}ve Bayes, Support Vector Machines or random forests as well.

Secondly, the concept of model applicability -- the question of when
the prediction of the model for a new object is reliable -- has grown
in importance with the increasing use of predictive models in
regulatory settings. A variety of methods have been developed that
attempt to characterize the domain of a model. These methods not only
determine whether models are applicable, but can also be used for
deciding if additional biological experiments are required for
reducing the prediction error on certain compound classes.

One of the key challenges faced by predictive modeling is the fact
that small molecules are not static and do not exist in
isolation. Traditionally, predictive models have focused on a single
structure for a small molecule and have ignored the role of the
receptor (in receptor-mediated scenarios). Yet, small molecules can
exist in multiple tautomeric forms and usually in multiple
conformations. As a result, enhancing the accuracy of predictions
ideally will require that such aspects be taking into account and, as
far as possible, take into account the receptor
simultaneously. Multi-conformer modeling has been addressed in the
4D-QSAR methodology described by Hopfinger and co-workers
\cite{Albuquerque:1998ys}.  Computer science techniques such as
\emph{multiple-instance learning} could also be applied to the
multi-conformer problem.

With the advent of high-throughput screening technologies, large
libraries of compounds can be screened against multiple targets in an
efficient manner. Such panel assays provide a broad, systems-level
view of small molecule activities.  Models developed on such data
afford us the opportunity to identify targets, characterize off-target
effects and so on. However, most approaches to this problem are rather
simplistic since they develop multiple individual models
\cite{Chen:2010zr}, leading to multiple, independent predictions for a
given input molecule.  Instead, one could imagine an approach that
takes into account the covariance structure of the multiple observed
activities and the structural descriptors within a single model. Such
an approach could lead to more robust predictions for panel
assays. Finally, this approach also better reflects clinically
relevant compound profiles~\cite{kuhn2010} and a ``personalized
medicine'' concept (\emph{e.g.}, drug-drug interaction profiles).

\subsection{Knowledge Management}
\label{sec:knowledge-management}

The relevance of scientific knowledge management is increasing not
only for reducing the risk in current research, but also for enabling
new collaboration/innovation opportunities with internal partners and
a rapidly growing number of external/public partners. In fact, in
cheminformatics and chemistry, scientists already switched many years
ago from paper lab-notebooks to online collaboration platforms, called
ELNs (Electronic Lab Notebooks). So, what were the drivers behind
chemists, and companies, adopting ``Enterprise 2.0,'' a social online
collaboration culture?

Before the year 2000, many chemists were still using paper
lab-notebooks, multiple compound registration, and search
tools. Compared to today there were many pitfalls and drawbacks
associated with this. The overall architecture was too inflexible to
adopt to fast changing data standards, scientists spent too much time
with administrative work, chemical data quality was inconsistent, an
alignment with other working groups was inefficient (\emph{e.g.}, for
running analytical experiments), a legal dispute required to find data
in paper lab-notebooks, data synchronization between systems caused
lag times and access delays, and local silos hampered an efficient
global collaboration. Now, chemists prefer ELNs, because they can run
searches in the data across the organization. This does not only allow
to find experts faster, but also allows access to highly trusted data
with approved quality workflows. Since this also gives access to
analytical and legal data directly every new ELN release is aiming at
increasing the integration of scientific and business workflows for
reducing the risk due to irrational or bounded-rational decision
making and increasing decision making efficiency.

ELNs are typically much more accepted and used within the
cheminformatics domain than in the bioinformatics domain, which is
picking up speed lately, the question being why this is the case. In
chemistry, every experiment requires a lot of expert
knowledge. Therefore, approaches for reducing risks by communicating
with fellow chemists about similar reactions or compounds is
critical. This requires capturing the reaction details, compound
details, and time consuming analysis details which ensure quality
data, such that these data can be searched by others.

It is worth mentioning that chemistry is traditionally a field where many people
have been used to huge bookshelves of reactions and compounds. Chemists changed
to online ELN collaboration not only to increase efficiency, but also
because it allowed collaboration with trusted colleagues. Third party paper
catalogs and academic publications proved to be
inefficient, and even more critically, they did not take compounds of trusted
colleagues into account. Overall the change of management to ELNs, similar to
``Enterprise 2.0,'' required delivering on the promise to increase
interconnectivity, and to improve collaboration
opportunities, and indeed it already performed these functions many years ago.

However, the future still leaves many challenges ahead. Using
external/public databases with chemical and bioactivity data remains a
challenge due to differences in ontologies, synchronization and
maintenance, and efficient substructure and similarity search within
complex data types (compounds). A collaboration with external parties,
e.g. contract research, poses other problems with duplicate checking
for compounds (e.g. tautomeric or salt structures) and efficient data
synchronization, and most often a double-registration is required for
fulfilling the data quality processes on both sides. If external
partners are small they often do not even have sufficient IT resources
themselves and thus rely on external services. Cloud services could
help not only to provide a service infrastructure for all parties
involved, but also to provide the required private/public access
toolbox. Furthermore, there are still many data sources not being
indexed properly, since chemistry patents are often cryptic and
image/text mining remains a puzzle. One could argue that the
image/text mining in scientific journals is less challenging; in fact,
it remains a huge challenge and many chemists do not trust automated
non-expert validated mining sources. The closed CAS (Chemical Abstract
Service) is a highly trusted source, and public chemistry and
bioactivity space has to improve quality and interconnectivity to
compare with it. Efforts like ChEMBL and PubChem-BioAssay are on the
right track. Still, improving data quality and standards between
public and closed sources will be absolutely critical for ensuring
constant growth, usage, and collaboration scenarios between private
and public parties.  In a business context we need to address ROI
(return-of-investment) questions, not only for single institutions and
companies, but also for collaboration networks of legal partners.  So,
only when the technical and scientific questions are solved can we
ensure that legal questions about contribution metrics will get enough
room to grow. Solving challenges is one thing (science and technical),
ensuring that the contributing partners get rewarded by their
contribution another (legal).

\section{Support for the Development of Novel Algorithms}
\label{sec:development-support}

For researchers interested in the development of new algorithms in the
field, a software library that handles chemical structures and related
data is essential. A wide variety of such chemical toolkits are
available, both commercial (which may offer free academic
licenses) and Open Source. 

Due to the importance of cheminformatics to the pharmaceutical
industry in particular, numerous commercial vendors provide libraries,
applications and database cartridges to handle a wide variety of
needs including Accelrys, BioSolveIT, ChemAxon, Chemical Computing Group,
Daylight, Open Eye, Schrodinger, Tripos, and Xemistry. More recently
there has been a growth in the development of Open
Source cheminformatics software, and this presents opportunities for
the rapid development and implementation of novel algorithms which can
build on the existing Open Source ecosystem. In this article we do not debate the
arguments for and against Open Source; we focus on Open Source
tools simply because this ensures that a reader will be able to explore
cheminformatics problems with a minimum of encumbrances.

The extent of open-source software in cheminformatics has somewhat
lagged behind related fields such as bioinformatics, partly due to the
existence of a lucrative pharmaceutical market for proprietary
software. However, in recent years there has been a rise in the amount
of open-source cheminformatics software. One contribution to this has
been the Blue Obelisk group~\cite{BlueObelisk2011}
(\url{http://blueobelisk.org}) of chemists, who have worked together
to create interoperable open-source chemistry software (among other
activities). Now there are a number of Open-Source chemical toolkits
available including the Chemistry Development Kit
(CDK)~\cite{steinbeck2003}, Open Babel~\cite{openbabel2011}, RDKit and
Indigo. These toolkits are written in Java or C++ (though toolkits
written in the latter invariably have bindings to a variety of
languages). Such toolkits are used, for example, to read/write
chemical file formats, manipulate chemical structures, measure
similarity of molecules, search for substructures, and generate 2D
diagrams. The underlying algorithms used include graph algorithms
(e.g. maximal common substructure, canonical labelling of graphs),
geometrical methods (e.g. Kabsch alignment) and vector manipulation
(e.g. converting coordinates in various systems, 3D structure
generation). As well as being of direct use to practising
cheminformaticians in their day-to-day work, many applications have
been developed that rely on these toolkits to handle chemical data;
for example, the molecular viewer Avogadro
(\url{http://avogadro.openmolecules.net}) uses Open Babel, while the
molecular workbench Bioclipse~\cite{Bioclipse2} uses the CDK.

The various toolkits have many features in common and at the same time
have certain distinguishing features. For example, the CDK implements
a large collection of molecular descriptors. On the other hand, Open
Babel has robust support for 3D structure generation and optimization
and supports the interconversion of a huge number of chemical file
formats. Table~\ref{tab:features} (in the Appendix) compares features
offered by the open-source toolkits currently available.  Given that
there is ample scope for software engineering (algorithm
implementation, code quality and analysis, build systems etc), both
the CDK and Open Babel are open to new contributions. Both are driven
via public, active mailing lists and public issue tracking systems.

It is important to note that certain challenges faced by nearly all
cheminformatics toolkits stem from the graph representation of
chemistry. This representation, while succinct and amenable to
computation, is only an approximation of reality -- and edge cases
abound. Existing areas of interest are enumeration of colored graphs,
taking into account symmetry, and symmetry detection itself, which not
merely derives from the chemical graph, but typically also from the 3D
geometry of the chemical. It is important to realize that although
chemical representations have increased in sophistication over the
years, many chiral molecules cannot be satisfactorily handled by
current representation systems. Perhaps a more challenging aspect is
that the chemical representation of a molecule really needs to capture
non-static graphs, in order to take into account delocalisation and
the tautomeric phenomena molecules can undergo in practice.

Of equal importance to the development of new algorithms is freely
available data on which to train and test new methods. As has been
noted, recently large structure and bioactivity data collections have
become available. As a result, this enables much more robust
validation of cheminformatics methodologies as well as large scale
\emph{benchmarking of algorithms}. The latter is especially relevant
for the plethora of data mining techniques that are employed in
cheminformatics. At this point, the nature of Open Data focuses
primarily on structure and activity data types. On the other hand,
there is a distinct lack of textual data (such as journal articles)
that are ``Open'' in nature. While PubMed abstracts serve as a proxy
for journal articles, text mining methods in cheminformatics are
hindered by not being able to mine the full text of many scientific
publications.  Interestingly, patent information is publicly
accessible (cf. Google Patents) and there have been efforts to mine
these for chemical information using Open tools.

%
\section{Conclusions}
\label{sec:conclusions}
Cheminformatics, the computer science of chemical discovery, is an
industrial and academic discipline with roots dating back to the
1960s, with the creation of DENDRAL, the first expert system of any
kind.  Much of the development of cheminformatics from the 1960s until
the new millenium was performed in secret, because of competition in
the pharmaceutical industry.  While chemists were among the first
scientists to embrace web-based tools of knowledge management,
research into fundamental algorithmic questions of structure
enumeration and database management (and a host of other topics) have
suffered, because of limited academic access to either tools or data.
As we enter the second decade of the 21st century, however, this
landscape is rapidly shifting.  There now exist free-of-charge
open-source cheminformatics toolkits, and open databases with tens of
millions of compounds.  We believe the prospects for academic and
industrial collaboration between chemists and computer scientists are
bright, and we would like to encourage more computer scientists to
participate in cheminformatics research.  There is an admittedly steep
learning curve, as the structural issues faced by chemists are often
too detailed and subtle to admit as clean an abstraction as
``algorithms on a string,'' the way many bioinformatics algorithms can
be abstracted into a computer science framework.  Nevertheless,
because of the changing landscape, there are many research questions
we are only now in position to try to answer: for a theorist,
graph-theoretic questions of three-dimensional enumeration; for a
database designer, more effective search algorithms and ontologies;
for a practical programmer, expansion of the many open-source
cheminformatics projects.  If more chemists think algorithmically, and
more computer scientists think chemically we are much better
positioned taking not only ``simple'' chemicals into account (only one
identifier and isomer possible per molecule), but also their complex
relationships, transformations, and combinatorial challenges.

In 2010, drug company Glaxo made public over 13,000 compounds that
they have investigated as potential cures for
malaria~\cite{glaxo-malaria}.  (Malaria has traditionally been a
neglected disease in drug research, because the populations who suffer
from it tend not to have enough money to be a stable consumer base for
a new pharmaceutical.)  Perhaps we will now be able to find cures for
some of these neglected diseases, by using the open source, open data
approach to the computer science of chemical discovery.

\section{Acknowledgements}
We would like to thank Danny Verbinnen for sharing his insights into
ELNs.  This article was written while A. Sterling was visiting the
Department of Electrical Engineering and Computer Science,
Northwestern University, supported in part by NSF grant
CCF-1049899. A. Bender thanks Unilever for funding.

\bibliographystyle{abbrv}
\bibliography{paper}

\appendix

\section{Glossary of Terms}
\begin{itemize}
\item \textit{Bioassay} - A system measuring the biological activity of a chemical compound on a particular
    biological assay (with all its parameters).
\item \textit{Biodegradation} - is the chemical dissolution of materials by bacteria or other biological means.
\item \textit{Conformation and conformational isomerism} - In chemistry, conformational isomerism is a form of
    stereoisomerism in which the isomers can be interconverted exclusively by rotations about formally single
    bonds.
    Single possible states of compounds are called conformers.
\item \textit{Ligand (biochemistry)} - a substance that binds to a protein.
\item \textit{Ligand (coordination chemistry)} - an atom, ion, or functional group that donates one or more of its
    electrons through a coordinate covalent bond to one or more central atoms or ions.
\item \textit{Mass spectrometry} - Mass spectrometry (MS) is an analytical technique that measures the
    mass-to-charge ratio of charged particles. It is used for determining masses of particles, for determining the
    elemental composition of a sample or molecule, and for elucidating the chemical structures of molecules, such
    as
    peptides and other chemical compounds.
\item \textit{Metabolites} - Metabolites are the intermediates and products of metabolism. The term metabolite is
    usually restricted to small molecules (chemical compounds).
\item \textit{Proteochemometric modeling} - combining data mining for chemical compounds (ligands) and proteins
    within the same machine learning framework.
\item \textit{Protonation and protonation state} - In chemistry, protonation is the addition of a proton (H$^+$) to
    an atom, molecule, or ion. The different states (different number of H$^+$ or on different atoms) are called
    protonation states for a molecular compound (e.g. de-protonated, mono-protonated, di-protonated, or any
    variations of those states).
\item \textit{QSAR} - Quantitative Structure-Activity Relationships; methods to relate the structure and the
    biological activity of a chemical structure in a quantitative manner.
\item \textit{Reaction catalysis} - Catalysis is the change in rate of a chemical reaction due to the participation
    of a substance called a catalyst.
\item \textit{Receptor (biochemistry)} - in biochemistry, a protein molecule that receives and responds to a
    neurotransmitter, or other substance.
\item \textit{Regulation authorities} - In the US the Food and Drug Administration (FDA) is responsible for
    protecting and promoting public health through the regulation and supervision of e.g. dietary supplements,
    prescription and over-the-counter pharmaceutical drugs (medications), vaccines, biopharmaceuticals, medical
    devices, and many other health related items. The European Medicines Agency (EMA) is a European agency for the
    evaluation of medicinal products.
\item \textit{SMARTS} - SMiles ARbitrary Target Specification \\(SMARTS) is a language for specifying substructure
    patterns in molecules
\item \textit{Stereoisomer/stereoisomerism} - Stereoisomers are isomeric molecules that have the same molecular
    formula and sequence of bonded atoms (constitution), but that differ only in the three-dimensional orientations
    of their atoms in space.
\item \textit{Tautomerism} - Tautomers are isomers (same molecular formula but different structural formula) of
    organic compounds that readily interconvert by a chemical reaction called tautomerization.
\end{itemize}
%
\section{Additional Figure and Tables}
%
\begin{table*}
\begin{tabular}{|l|l|l|} \hline
\textbf{Property} & \textbf{Requirement} & \textbf{Potential of Cheminformatics Approaches} \\ \hline
\multicolumn{3}{|l|}{\textbf{Research Stage}} \\ \hline
\multirow{3}{*}{Bioactivity} & Compound is active & \textsc{High} -- computer models can often predict \\
& in \emph{in vitro} (``test tube'')
studies &  which compounds are bioactive, \\
&& the so-called \emph{virtual screening} approaches \\ \hline
\multirow{3}{*}{Solubility} & Compound is soluble  & \textsc{High} -- computer models can often predict \\
& at effective concentration & which compounds are insoluble, \\
&& decreasing the need for experimental testing \\ \hline
\multirow{3}{*}{No Obvious Toxicity} & No known ``toxicophores''  & \textsc{High} -- approaches such as substructure
searching \\
& contained within molecule & are routinely applied to remove
chemical structures \\
&& with functional groups that
are known to be toxic \\ \hline
\multirow{3}{*}{Selectivity} & Compound does not modulate  & \textsc{Medium} -- \emph{in silico} approaches can often make
guesses  \\
& other than  & which proteins are modulated by a
molecule, but the \\
& the intended protein
target & precise prediction of
bioactivity profiles remains a challenge \\ \hline
\multicolumn{3}{|l|}{\textbf{Development Stage (All of the above, plus...)}} \\ \hline
\multirow{3}{*}{Efficacy} & Compound is not only
active
& \textsc{Low} -- bioinformatics approaches can partially
be used  \\
 & in the test tube, but also & to anticipate whether modulation of a
particular protein  \\
in Animal/Human Models & in the
organism  & shows an effect in humans via pathway modeling\\ \hline
\multirow{3}{*}{Safety in} & Compound exhibits
tolerable & \textsc{Low} -- bioinformatics approaches can partially
be used \\
& side effects in
animal
models & to anticipate safety concerns in
humans, \\
Animal/Human Models & and/or humans & however experimental confirmation is
required \\ \hline
\multirow{4}{*}{Bioavailability} & Compound gets
dissolved & \textsc{Medium} -- cheminformatics models for
bioavailability  \\
& in the
gastrointestinal tract & can be generated, however due
to the complexity  \\
& and it is absorbed into
the  & of the process they are in
many cases  \\
& bloodstream & not sufficiently reliable for practical
use \\ \hline
\multicolumn{3}{|l|}{\textbf{Marketed Drug (All of the above, plus...)}} \\ \hline
\multirow{3}{*}{Cost of Synthesis} & The compound can be
 & \textsc{Medium} -- some cheminformatics approaches
exist \\
& synthesized at a  & that can estimate the difficulty of \\
& reasonable cost & synthesizing a particular chemical structure \\ \hline
\multirow{2}{*}{}Better Efficacy & The compound has & \textsc{Medium} -- the profile of the final product will
determine  \\
than Competitor Products & a market advantage & the particular advantage of the drug
in the market \\ \hline
\multirow{4}{*}{} & Patients with a disease & \textsc{Medium} -- business decisions are quite relevant \\
Existence of  & treated by the drug exist & when drugs are developed in private pharmaceutical \\
Relevant Market Need   & and they possess the means  &  companies (but often to a lesser extent \\
& to buy the drug &  in government-owned pharmaceutical research laboratories) \\  \hline
\end{tabular}
\caption{Some of the properties potential drug candidates need to have to be considered as ``promising,'' either in the different research and development stages, as well as a drug that is finally admitted to the market. Note that the particular requirements are also heavily project-dependent, and that also in cases where cheminformatics approaches can be employed in drug design an experimental confirmation of the prediction is required -- the computer can hence decrease the experimental burden, but not replace it altogether.}
\label{table:properties}
\end{table*}
%
\begin{table*}
  %\onehalfspacing
  \centering
  \begin{tabular}{lcccc}  \hline
    \textbf{Feature} & \textbf{CDK} & \textbf{OpenBabel} &
    \textbf{RDKit} & \textbf{Indigo}\textsuperscript{1} \tabularnewline \hline
    License & LGPL & GPL & new BSD & GPL or commercial \tabularnewline
    Language & Java & C++ & C++ & C++ / Python \tabularnewline
    SLOC\textsuperscript{2} & 188,554 & 194,358 & 173,219 & ? \tabularnewline
    Structure-based Fingerprints & \lots\textsuperscript{3} & \lots & \lots & yes \tabularnewline
    %\ \ \ Substructure & \lots & \lots & \lots \tabularnewline
    Support for multiple chemical file formats & \little & \lots & \least & \little \tabularnewline
    %Aromaticity models & \least & \least & \least \tabularnewline
    %Stereochemistry & \least & \little & \lots \tabularnewline
    Canonical labeling of graphs & \lots & \lots & \lots & yes \tabularnewline
    Calculation of molecular descriptors & \lots & \least & \lots & no \tabularnewline
    %2D coordinate generation & \lots & \none & \lots & no \tabularnewline
    3D coordinate generation & \least & \lots & \lots & no \tabularnewline
    2D depictions & \lots & \little & \lots & \little \tabularnewline
    Generation of conformational isomers & \none & \least & \least & no \tabularnewline
    %Rigid alignment & \lots & \lots & \lots \tabularnewline
    SMARTS searching/matching & \lots & \lots & \lots & yes \tabularnewline
    %Pharmacophore searching & \little & \none & \lots \tabularnewline
    Chemical reaction enumeration & no & no & no & yes \tabularnewline
    \hline
    \multicolumn{5}{l}{1. Indigo is a new toolkit (published late 2010), and we have not investigated its features in depth.} \\
    \multicolumn{5}{l}{2. Source Lines of Code as measured by the tool \emph{sloccount} (\url{http://www.dwheeler.com/sloccount/}).} \\
    \multicolumn{5}{l}{The count includes all source files, in any language, for the
  project. In addition, only the trunk} \\
    \multicolumn{5}{l}{for each project was considered.  We did not count Indigo's lines of code.} \\
    \multicolumn{5}{l}{3. More checks indicates more complete implementation of a feature.} \\
  \end{tabular}
  \caption{Cheminformatics features provided by open-source toolkits.}
  \label{tab:features}
\end{table*}

\begin{figure}
\centering
\includegraphics[height=1.3in]{platins.png}
\caption{The generated InChI is the same for the stereoisomers cisplatin and transplatin.}
\label{figure:cistrans}
\end{figure}
%
\begin{figure}
\centering
\includegraphics[height=2in]{CACM_ApplicabilityDomain.pdf}
\caption{Illustration of the ``Applicability Domain'' concept. In order to estimate whether model predictions are reliable, it is crucial to define areas of ``chemical space'' where the model is applicable, and where it is not. In this case, the ``model domain'' includes the molecule at the top. The (relatively similar) molecules to the left are likely to be included in the Applicability Domain of the model, while the (more dissimilar) molecules to the right are likely located outside this domain, hence the generated model is probably not applicable.}
\label{figure:applicability-domain}
\end{figure}
%
\end{document}
