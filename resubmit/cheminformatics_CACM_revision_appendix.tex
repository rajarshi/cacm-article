\documentclass{sig-alternate}

\begin{document}

\title{Cheminformatics: The Computer Science of Chemical Discovery: Appendix}
\numberofauthors{9}
\author{
\alignauthor
Joerg Kurt Wegner\\
       \affaddr{Tibotec BVBA}\\
       \affaddr{Turnhoutseweg 30}\\
       \affaddr{2340 Beerse Turnhout, Belgium}\\
       \email{jwegner@its.jnj.com}
% 2nd. author
\alignauthor
Aaron Sterling\\
       \affaddr{Department of Computer Science}\\
       \affaddr{Iowa State University}\\
       \affaddr{Ames, Iowa, USA}\\
       \email{sterling@iastate.edu}
% 3rd author
\alignauthor
Rajarshi Guha\\
\affaddr{NIH Center for Translational Therapeutics}\\
\affaddr{9800 Medical Center Drive}\\
\affaddr{Rockville, MD 20850}\\
\email{guhar@mail.nih.gov}
}

\additionalauthors{Additional authors:
Andreas Bender (University of Cambridge, email: {\texttt{andreas.bender@cantab.net}}),
Jean-Loup Faulon (University of Evry, email: {\texttt{Jean-Loup.Faulon@issb.genopole.fr}}),
Janna Hastings (European Bioinformatics Institute, Cambridge, UK, email: {\texttt{janna.hastings@gmail.com}}),
Noel O'Boyle (University College Cork, Cork, Ireland, email: {\texttt{baoilleach@gmail.com}}),
John Overington (European Bioinformatics Institute, Cambridge, UK, email: {\texttt{jpo@ebi.ac.uk}}),
Herman Van Vlijmen (Tibotec, Beerse, Belgium, email: {\texttt{hvvlijme@its.jnj.com}}), and
Egon Willighagen (Karolinska Institutet, Stockholm, Sweden, email: {\texttt{egon.willighagen@ki.se}})
.}
\date{25 June 2011}
\maketitle

\appendix
%
\section{History of Cheminformatics}
%
\subsection{Definitions and References}
The aim of this brief review of the history of cheminformatics is to
put the content of the main article into a broader perspective. If we
consider all information, analysis, and \emph{in silico} optimization
of a molecule as ``cheminformatics,'' then the field is very large,
since a molecule plays the central role in many related
disciplines. Clearly, this article is not meant to be an exhaustive
overview; rather, we point the reader to more detailed references and
highlight some historic milestones.  In addition, we also discuss
some terms as they relate to cheminformatics.  It is thus useful to
first provide some definitions of cheminformatics itself, given that
they also shed some light on the founding principles of the field. For
example, we have the following definitions:
\begin{itemize}
\item ``\textit{The mixing of information resources to transform data
    into information, and information into knowledge, for the intended
    purpose of making better decisions faster in the arena of drug
    lead identification and optimization.}'' Frank K. Brown, 1998.
\item ``\textit{[Chemoinformatics involves]... the computer
    manipulation of two- or three-dimensional chemical structures and
    excludes textual information. This distinguishes the term from
    chemical information, largely a discipline of chemical librarians
    and does not include the development of computational methods.}''
  Peter Willett, 2002.
\item ``\textit{\ldots the application of informatics to solve chemical
    problems.}'' and ``\textit{\ldots chemoinformatics makes the point that
    you're using one scientific discipline to understand another
    scientific discipline.}'' Johann Gasteiger, 2002
\item ``\textit{The set of approaches to computer-aided drug design
    that do not rely on the 3D structure of the [protein] target}''
  John Van Drie, 2011 (personal communication)
\end{itemize}
More generally, Chen \cite{Chen2006} and Brown \cite{brown2009} cited various
definitions of pioneers in the field with Brown concluding that
``'\textit{The differences in definitions (of the term
  cheminformatics) are largely a result of the types of analyses that
  particular scientists practice and no single definition is intended
  to be all-encompassing},'' as is evident from the three definitions above.
It is also useful to clarify some other terms and how
cheminformatics might relate to them:
\begin{itemize}
\item \textbf{Quantum chemistry} (QM, where ``M'' stands for ``mechanics'') is generally associated with
  theoretical chemistry or chemical physics. Briefly, it focuses on
  the description of chemical systems starting from first principles
  and is based on the Schr\"{o}dinger wave equation and frameworks
  built on top of it. The use of quantum mechanics allows one to
  describe molecules in terms of electron density, and hence calculate
  molecular properties (spectra, energies, etc.) in a physically
  accurate manner.  The results of quantum chemistry calculations are
  used in molecular modeling (energy force field parameters) and
  cheminformatics (atom partial charges, or full molecule properties,
  e.g., molecule polarizabilities).
\item \textbf{Molecular modeling} or \textbf{Computational chemistry}
  can be considered an approximation of quantum chemistry and aims to
  evaluate many of the properties considered by QM methods. The key
  difference is that these methods employ a variety of
  parameterizations (usually, but not always, derived from QM studies),
  allowing one to evaluate energies for small molecules very rapidly
  as well as to handle larger systems (such as proteins) that are too
  time consuming for QM methods. A variety of techniques including
  semi-empirical methods, molecular mechanics and docking belong to
  this category. While it is true that many of these methods involve
  cheminformatics concepts (choice of tautomer, partial charge
  assignments, etc.), the methodologies are fundamentally physical in
  nature. We note that computational chemistry has close ties to
  topics such as x-ray crystallography and NMR structure elucidation.
\item \textbf{Chemometrics} is a focused application of statistical
  methods to problems in analytical chemistry, such as infrared, mass
  and NMR spectroscopy.
\item \textbf{Translational research/medicine} aims to make more
  direct links between academic (or early stage) research and clinical
  practice in an effort to speed up the results of research into
  actual treatments. When applied to medical research this usually
  implies efforts to speed up the process by which a compounds goes
  from the lead stage to the clinical trial stage. Naturally,
  cheminformatics plays a role at various stages of this process (lead
  identification, lead optimization, ADMET modeling) as well as more
  broadly such as in drug repurposing
  efforts \cite{Dudley:2011fk,Swamidass:2011uq}. Related topics and
  books are also Pharmacogenomics \cite{yan2008pharmacogenomics} and
  Chemogenomics \cite{kubinyi2004chemogenomics}.
\end{itemize}
Obviously, we cannot cover all domains that involve or refer to
cheminformatics concepts. However, there are a number of publications
that cover cheminformatics topics in different ways.  Probably the
most comprehensive cheminformatic treatise was published by Gasteiger
\cite{Gasteiger2003} in 2003.  More concise, introductory texts
followed from Gasteiger \& Engel \cite{gasteigerengel2003} and Leach
\& Gillet \cite{leachgillet2007}.  The books of Bajorath
\cite{Bajorath2004} and Oprea \cite{oprea2005} focus on the
applications of cheminformatics in drug design and span a number of
topics. The first books discussing chemical graph theory and
algorithms were published in 1989 by Zupan \cite{zupan1989}, 1991 by
Bonchev and Rouvrey \cite{bonchevrouvrey1991,bonchevrouvrey2003} and
1992 by Trinastic \cite{Trinajstic1992}.  More recently, Faulon \&
Bender \cite{faulon2010} have edited a collection focusing on
algorithms in cheminformatics, including various applications of graph
theory.  Other books focus more on specific topcs such as mathematical
challenges \cite{mathchallenges1995}, molecular diversity
\cite{moleculardiversity1999}, factor analysis \cite{Malinowski2002},
evolutionary algorithms \cite{clark2000} and molecular descriptors
\cite{todeschini2000}.

\subsection{Historical Milestones}
In this section we highlight the development of a variety of
cheminformatics concepts and techniques over time. The main journal for this field
(\textit{Journal of Chemical Documentation} was founded in
1961. Interestingly, the journal was renamed to \textit{Journal of
  Chemical Information and Computer Sciences} in 1975 to reflect the
tight connection between chemical information and computer
science. The term
``chemoinformatics'' was coined by Brown in
1998\cite{brown1998}.  Many authors, including us in this article, drop the ``o'' and use the word ``cheminformatics''; the words can be used interchangeably.  The Chemical Abstract Service (CAS) of the American Chemical
Society (ACS) formed a research department in 1955 and starting from
1965 they provided their Chemical Registry System, and in 1968 they
made the first computer-readable file of all abstracted documents
available \cite{Chen2006}.

From a more technical viewpoint, the first chemical graphs were drawn
by the Scottish chemist Willam Cullen in 1758
\cite{bonchevrouvrey1991}, who initially called them affinity graphs.  Many fundamental principles of graph theory and
combinatorics were developed in the context of counting isomers of
paraffin by Cayley~\cite{CayleyTrees}, Polya~\cite{PolyaEnumeration} and others.
After developing the concept of bonds between atoms (Couper, 1858) the
first chemical graphs occured in publications of Brown in 1864 and
Cayley in 1874 \cite{bonchevrouvrey1991,brown2009}. The notion of
mathematical chemistry in general was discussed by Helm in 1897
\cite{Helm:1897ys} and the reader is referred to Balaban
\cite{Balaban:2005zr} for a historical overview of this field.

The year 1946 may be regarded as the birth year of cheminformatics
\cite{Chen2006}: \textit{``In 1946 King et al.\cite{kct1946} published
  an article illustrating the use of IBM's business accounting
  machines in carrying out the construction of the rotational spectra
  of asymmetric rotors by the evaluation of mathematical equations for
  line position and line intensity''}. One could argue that this is
more of a computational chemistry application.

From an informatics perspective, the first record of managing chemical
information goes back to the chemical literature being indexed since
the year 1771.  In 1881 the first edition of the \textit{Beilstein's
  Handbuch der Organischen Chemie} encyclopedia was published
\cite{polanski2009} registering 1500 chemical compounds.  Many key
cheminformatics algorithms appeared in the early to mid 20th century
(though in some cases, the underlying mathematics was known much
earlier). Ray and Kirsch published in 1957 the first substructure
searching algorithm to support retrieval of computerized structure
records \cite{RayKirsch1957}. Subsequent developments improved
substructure searches, notably the use of pre-screening via fragments
\cite{Adamson:1973fk,Feldman:1975uq} in the 1970's. One of the key
algorithms for molecular graph canonicalization is the Morgan
algorithm \cite{Morgan1965}, described in 1965.

In 1965 the DENDRAL expert system started with the aim to
automatically determine the structure of an unknown chemical compound
from the corresponding mass spectrum \cite{Gray1986}. The system is
also often cited as one of the earliest artificial intelligence
and expert systems \cite{Chen2006}.

The first system supporting chemical synthesis planning was OCSS
(Organic Chemical Simulation of Syntheses) developed by Corey and
Wipke in 1969 \cite{CoreyWipke1969} which in turn was an
implementation of theories described by Vleduts in 1963
\cite{Vleduts:1963kx}.

QSAR is one of the most well known applications of predictive
cheminformatics and originates with the work of Hansch
\cite{Hansch:1962vn} in 1962. While Hansch's approach focused on
property-activity relationships, true structure-activity relationships
originated with the work of Free and Wilson in 1964
\cite{Free:1964ys}. Subsequent work in this area led to 3D-QSAR
techniques including CoMFA \cite{Cramer:1988zr} and
CoMSIA\cite{Klebe:1994ly}.

In 1978 Gasteiger and Marsili published a fast algorithm to calculate
partial charges in organic molecules by a Partial Equalization of
Orbital Electronegativities (PEOE) \cite{gm78}, which even today
is considered one of the gold-standards to calculate partial charges of atoms.

Instead of converting molecular graphs to a vector representation for
applying machine learning methods, it is also possible to use direct
molecular graph mining methods \cite{okada2006}. Some of the first
published methods use neural networks \cite{kireev1995}, inductive
logic programming \cite{yh02a}, or graph kernels \cite{kti03}.
%
\section{Open Questions for Computer Science and Cheminformatics}
%
We offer the following open questions to suggest concrete interdisciplinary research directions for computer scientists working with chemoinformaticians.  This list is by no means exhaustive.  We have focused on questions that, in our opinion, are answerable, and whose answers would be of great interest to the field.  The questions are grouped roughly by computer science subarea, and include both open theoretical problems and ``requests'' for open-source practical implementations.  We hope CS researchers new to cheminformatics find this useful.

\subsection*{Algorithmic graph theory}
\begin{enumerate}
\item \emph{Design an algorithm that approximately counts the number of (3D) conformers of a chemical formula.}

  This question encompasses the open questions of \emph{approximately
    counting the number of stereoisomers, or the number of tautomers,
    of a chemical formula}.  Goldberg and Jerrum designed an algorithm
  which, given a chemical formula, would output an isomer of that
  formula chosen uniformly at random~\cite{RandomlySampling}.  Perhaps
  the main theoretical obstacle to overcome is that molecules tend to
  be \emph{chiral} (that is, they have a ``handedness'' or 3D
  orientation), whereas traditional graph theory (and the
  Goldberg/Jerrum algorithm) treats graphs with identical vertices and
  edges as isomorphic.  So part of the question could be rephrased as,
  ``Given a labeled vertex set, count the number of structures that
  are identical on that vertex set, except that they differ in their
  3D orientation.''  Mathematical chemists have designed measures of
  molecular chirality~\cite{ChiralityMeasures}. A resolution of this
  problem may require connecting graph enumeration algorithms to knot
  theory or other topics in topology~\cite{TopologicalLook}.
%
\item \emph{Design and implement efficient subgraph isomorphism
    algorithms for useful special cases of molecular graphs}.

  The Subgraph Isomorphism Problem is known to be
  $\textsf{NP}$-complete (hence possibly harder than the Graph
  Isomorphism Problem, which is not known to be
  $\textsf{NP}$-complete).  Nevertheless, finding maximum common
  subgraphs to match chemical structures is of fundamental importance
  in chemistry; hence, much work has been invested in partial
  solutions to this problem.  Raymond and Willet reviewed the state of
  the art in 2002~\cite{MCSreview}.  As one possible way to attack
  this problem, we note the empirical fact that molecular graphs are
  of bounded degree, and are observed to have \emph{treewidth} $\leq
  5$~\cite{treewidth}.  Bounded treewidth is at least theoretically
  useful~\cite{Epp-JGAA-99}, and it may be possible to improve on
  current open-source implementations whose isomorphism-checking
  routines are written for all graphs, instead of taking advantage of
  special properties of chemical graphs.

\item \emph{Searching within complex data types, e.g. molecules, for semantic web approaches}.

  One key concept of the linked data web, the semantic web, is that
  different data sources can be readily integrated with each
  other. Still, in the field of Cheminformatics, we are not only
  interest in linking two molecules (the linking normalization problem
  for different protomers, tautomers, or special cases of isomerisms
  remain open), but we are also interested in being able to search
  efficiently within molecules when being linked via semantic web
  approaches. Typical searches will require being able to apply
  substructure or similarity searches.  What could be algorithmic
  solutions for this?
\end{enumerate}

\subsection*{Cryptography}

Currently, companies simply cannot submit any chemical queries over web services, because it is not possible to ensure the web-service is not keeping a log, and not possible to prevent the service from accidently publishing a compound after five years of research (thus killing a drug from entering the market).  There are some papers that address cryptographic problems in chemistry (for example,~\cite{Digital_Watermarking_Chemicals}), but the state of the art is unsatisfactory.  How might one encrypt molecules so they could only be used over the web as a black box?  Beyond that, how might one attach security levels ensuring that only very specific and certified algorithms would be allowed on the molecule, or licensing, collaboration conditions, etc.?

\subsection*{Data mining \& machine learning}
\begin{enumerate}


\item \emph{Small molecules \& phenotypic data}

Phenotypic screens (where one takes images of cells and then analyzes them,
extracting numerical features from the image) are increasingly
common. These types of screens lead to very large, very high
dimensional datasets.
\begin{itemize}
\item What methods are suitable to perform feature selection and
  modeling of such data?
\item Do phenotype-derived data lead to ``better'' models for
  small molecules compared to the usual structure-derived data?
\end{itemize}

\item \emph{Inverse QSAR (or de-novo design)}

(Statistical) QSAR is an example of traditional data mining where one
correlates a set of independent variables to a dependent
variable. This lets one predict properties of a new molecule based on
its structural features and those of a training set. The \emph{inverse QSAR
problem}, also called \emph{de-novo design}, is: given a molecule structure representation
(usually a descriptor vector), what are the possible input structures
that satisfy the representation. This process might be considered as
a chemical design of experiments. It is clearly a non-continuous optimization problem, since
not all chemical molecules might be accessible, and cost/risk to create molecules physically is another
critical factor. This can be made more complex, by
asking that the input structures also satisfy an experimental property
range.
\begin{itemize}
\item What methodologies can be devised to address this problem?
\item How can we ensure combinatorially created suggestions make use of the large
chemical structure, chemical building block, and chemical reaction databases by suggesting
chemically feasible molecules (not just virtually accessible ones)? How can we improve
synthetic accessibility predictions \cite{Boda_Seidel_Gasteiger_2007}?
\item How does a given methodology allow us to translate the
  descriptor space to a minimal region of the input space?
\item Are there internal features of a modeling algorithm (say
  hyperplanes in a SVM approach) that let us simplify the problem?
\end{itemize}

\item \emph{Dynamic similarity search on instant binary vectors}.

  Binary feature vectors are a common practice for chemical similarity
  searches. The typical process starts with 1. creating binary
  substructure (or other feature vectors) \cite{citeulike:8530538},
  2. creating fixed-length binary vectors of typically 1024 bits for
  reducing space requirements and speeding up further similarity
  searches (by losing some accuracy), 3. creating further
  pre-computations for speeding up threshold-based similarity searches
  \cite{doi:10.1021/ci800076s}.\\  Still, if we would like to employ
  dynamic changes in the similarity encodings, e.g. using only a set
  of binary features, then pre-computations like hashing
  might need to be redone efficiently on the fly. Finally, the major
  goal is to employ similarity searches \cite{doi:10.1021/ci200235e}
  on a scale of multiple million entries and more optimizations and
  benchmarking studies are urgently required, e.g. using GPUs
  \cite{doi:10.1021/ci1004948}, or optimizing pair-wise similarity
  calculations \cite{MINF:MINF201100050}.
%

\item \emph{Chemical image/text mining in patents (curation)}.

  There are various tools for doing automatic text mining on chemical
  patents. Still, the overall acceptance rate of chemical text mining
  is improvable, since many medicinal chemists are very concerned
  about the data quality of such efforts.\cite{chemicalEntityRecognition,textMiningForDrugs}

  \begin{itemize}
  \item What could be done to improve the mining quality, to curate the
    obtained data, and to provide confidence level estimations for
    each molecule coming from patent mining?
  \item Do image2structure and text2structure mining also require data
    stores for ensuring a sufficient amount of confidence and data
    quality?
  \item How can patent mining be used to create new drugs faster or to
    speed-up collaboration/licensing discussions?
 \end{itemize}

\item \emph{Large-scale vectorial versus kernels molecule similarity}

  Vectorial molecule encodings can serve as efficient approximations
  of molecules.  Sometimes non-vectorial molecular 3D shape or
  molecule kernel comparisons might be more suitable to compare
  molecules, since they might better correlate with activities. One
  key problem is that non-vectorial encodings require comparison of all
  molecules (or their 3D conformational explosions) in a pair-wise
  manner.  This becomes prohibitively expensive when considering
  millions of molecules.  Can dyadic data approaches help
  \cite{Hochreiter:2006:SVM:1159508.1159516}? What about other approximations or
  cascading flows?
%
\item \emph{Using multiple annotations for improving molecular mining/predictions (chemogenomics)}

  As an example: Biological activities might not be independent of
  each other, but have a certain correlation between each other.  In
  Chemogenomics this is used to create models of combining
  molecules with protein sequences, molecules with active sites of
  proteins, or molecules with biological activities of multiple
  assays. How can we optimize such highly complex mining scenarios,
  especially when considering large-scale data sets with hundreds of
  thousands of molecules and thousands of biological activities?  How can
  we combine, mine, and visualize categorial and continuous output
  variables, e.g. hydrophobicity of a molecule and toxicity in humans,
  by still being able to make concrete proposals to medicinal
  chemistry? Is analoging (creating very small modifications of a
  molecule and measuring its activities) really the most efficient way
  forward? If we test molecules, should we test it in a single
  biological assay or in multiple biological assays, and, if multiple,
  which ones?  If a company does not have a biological assay within
  reach, which other partner could offer testing a molecule within two
  days (vendor matching based on licenses or contracts)?
\end{enumerate}

\subsection*{Databases \& software engineering}
\begin{enumerate}


\item \emph{Real time substructure searching in massive chemical
    databases}.

  Chemical databases have grown tremendously in size. A common task in
  such databases is substructure searching. However most cases of very
  large structure databases do not support substructure in real time
  (something that would allow applications such as
  ``type-ahead''). How can one enable such rapid substructure searches
  on massive ($> 10^9$ molecules) collections of structures? This
  problem has various aspects:
  \begin{itemize}
  \item What type of indexing schemes will support rapid substructure searches?
  \item How can other molecular properties be included in substructure
    searchs that allow ranking of results, all the while maintaining
    real-time response (i.e. $< 1$ sec)?
  \item What type of database architectures are required for this
    scale of structure searching?
  \end{itemize}

\item \emph{Large scale conformational databases}.

  While many programs are available to generate 3D conformations, on
  the fly generation for large collections can be time
  consuming. Rather, can we store massive conformer collections and
  support 3D searches over them? Beyond generating the conformers
  themselves, this problem has several aspects covering database
  design, parallel systems and software engineering:
  \begin{itemize}
  \item How many conformers are required to provide \emph{sufficient} coverage?
  \item What representation will be used to store conformers and run
    queries? How does the choice of representation affect the type of
    queries we can run?
  \item Most 3D similarity approaches either employ a vectorial
    representation or a volumetric representation. How can these be
    efficiently indexed?
  \item What type of parallel infrastructure can be used to speed up queries?
  \end{itemize}


\item \emph{Database indexing schemes for chemical representations}.

Databases of chemical structures are ubiqituous, employing a variety
of RDBMS products. Structure-based queries (exact match, substructure,
similarity) are dependent on the chemical representation---most
solutions employ a linear string based form (SMILES, InChI) and
depending on the nature queries to be supported, some are preferred
over others. While one can perform linear scans, indexing is key to
efficient query performance.
\begin{itemize}
\item  Since many queries use binary fingerprints as pre-screens, what
  types of indexing schemes can be designed to support queries on
  binary vectors?
\item If we choose to support 3D searches (shape, pharmacophore) what
  indexing scheme will allow us to perform these types of queries
  efficiently?
\end{itemize}

\item \emph{Map/Reduce in cheminformatics}

  Many cheminformatics tasks apply algorithms over large input files
  or across many molecules (similarity and substructure search,
  docking, filtering). A simple way to parallelize this is to chunk
  the input data and let individual threads/nodes process each chunk. A
  trivial solution is to manually chunk the input and submit a series
  of jobs to a scheduling system. Using the Hadoop ecosystem as an
  example, we can ask various questions:
  \begin{itemize}
  \item Can we employ modern frameworks like Hadoop to support
    embarassingly parallel cases (filtering $10^7$ molecule libraries)?
  \item Can we develop a generalized framework that includes the
    requisite cheminformatics tools that allows users to seamlessly
    distribute jobs over Hadoop and other map reduce systems?
  \item Is the latency involved in Hadoop-based datastores (HBase)
    worth the ability to handle massive molecule collections and run
    M/R queries across them?
  \item Going further, are there cheminformatics problems that can make use
  of the map/reduce paradigm at the algorithmic level?
  \end{itemize}
\end{enumerate}

\subsection*{Enterprise software (KM,ELN)}
We know that the enterprise software and ELN market is still growing.
\begin{enumerate}
\item \emph{Public-private collaboration and security scenarios}

  Let us assume an organization, e.g. a commercial company, has a
  single or a small number of established KM and ELN products.  How
  can we improve the maintenance, leveraging, and collaboration with
  many external partners (each of them potentially with a distinct KM/ELN
  solution)? Which party is hosting which data in which data structure
  (ontologies?), and how can we ensure that only pre-defined data
  entries (and a limited number of annotations, e.g. biological
  activities) are visible to a partner?  How can this be organized for
  a multitude of partners?  How best to incorporate cloud computing, user management,
  encryption granularity and efficient security management?

\item \emph{Licensing in a parallel world} Many software vendors use
  different solutions for parallelizing computing jobs: SGE, PVM, MPI,
  etc.  Is a cloud really an option?  What about SaaS with secured data
  transfer?  Can this also offer alternative licensing strategies for
  software suites in this domain?


\end{enumerate}
%
\bibliographystyle{abbrv}
\bibliography{resubmission}

\end{document}
